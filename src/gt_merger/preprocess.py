"""
/*
 * Copyright (C) 2019-2021 University of South Florida
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 """
import datetime

import pandas as pd
import pytz

from src.gt_merger import constants


def is_valid_oba_dataframe(df_csv):
    """
    Validate if the oba dataframe is not empty and includes the required columns to perform the preprocess
    :param df_csv: Dataframe loaded from the oba exported csv file
    :return: True if df_csv is not empty and includes the required columns, False otherwise
    """
    # Validate df is not empty
    if df_csv.empty:
        return False
    # Validate existence of relevant columns
    if not set(constants.OBA_RELEVANT_COLS_LIST).issubset(df_csv.columns):
        return False

    return True


def is_valid_gt_dataframe(df_gt):
    """
    Validate if the ground truth dataframe is not empty and includes the required columns to perform the preprocess
    :param df_gt: Dataframe loaded from the ground truth data file
    :return: True if gt_csv is not empty and includes the required columns, False otherwise
    """
    # Validate df is not empty
    if df_gt.empty:
        return False
    # Validate existence of relevant columns
    if not set(constants.GT_RELEVANT_COLS_LIST).issubset(df_gt.columns):
        return False

    return True


def preprocess_oba_data(data_csv, min_activity_duration, min_trip_length, remove_still_mode,
                        min_data_collection_date, max_data_collection_date) -> object:
    """ Preprocess the csv data file from oba-firebase-export as follows:
        - Change activity start date datatype from str to datetime
        - Drop observations whose activity start date are NaN after data type conversion
        - Drop observations that does not match the time duration and distance requirements from command_line_args
          minActivityDuration and minTripLength
        - Add column required to be used as key while merging with ground truth data

    :param max_data_collection_date: Upper limit date to filter oba data by Start date
    :param min_data_collection_date: Lower limit date to filter oba data by Start date
    :param data_csv: A data frame loaded from a csv file generated by oba-firebase-export
    :param min_activity_duration: numeric value representing minimum valid activity duration in seconds.
    :param min_trip_length: numeric value representing minimum valid trip distance in meters.
    :param remove_still_mode: boolean value to indicate if records with STILL mode must be removed
    :return: Preprocessed dataframe
    """

    # Assure that 'Activity Start Date and Time* (UTC)' is datetime
    data_csv['Activity Start Date and Time* (UTC)'] = pd.to_datetime(data_csv['Activity Start Date and Time* (UTC)'],
                                                                     errors='coerce', utc=True)

    #  Filter oba data using min and max activity start date and time
    data_csv = data_csv[(data_csv['Activity Start Date and Time* (UTC)'] >= min_data_collection_date) &
                        (data_csv['Activity Start Date and Time* (UTC)'] <= max_data_collection_date)]

    # Assure that 'Origin location Date and Time (*best) (UTC)' is datetime
    data_csv['Origin location Date and Time (*best) (UTC)'] = pd.to_datetime(data_csv['Origin location Date and Time '
                                                                                      '(*best) (UTC)'],
                                                                             errors='coerce', utc=True)

    # Assure that 'Activity Destination Date and Time* (UTC)' is datetime
    data_csv['Activity Destination Date and Time* (UTC)'] = pd.to_datetime(
        data_csv['Activity Destination Date and Time* (UTC)'],
        errors='coerce', utc=True)

    # Assure that 'Destination Location Date and Time (*best) (UTC)' is datetime
    data_csv['Destination Location Date and Time (*best) (UTC)'] = pd.to_datetime(
        data_csv['Destination Location Date and Time (*best) (UTC)'],
        errors='coerce', utc=True)

    # Remove records with STILL mode if required
    if remove_still_mode:
        clean_data_csv = data_csv[data_csv['Google Activity'] != 'STILL']

    # Drop NaN rows for relevant columns
    clean_data_csv = clean_data_csv.dropna(
        subset=constants.OBA_RELEVANT_COLS_LIST)

    # Keep only trips with Duration greater or equal than command_line_args.minActivityDuration minutes and distance
    # greater of equal than command_line_args.minTripLength
    clean_data_csv = clean_data_csv[(clean_data_csv['Duration* (minutes)'] >= min_activity_duration) & (
            clean_data_csv['Origin-Destination Bird-Eye Distance* (meters)'] >= min_trip_length)]

    # Add the data to be dropped to a data frame
    data_csv_dropped = pd.merge(data_csv, clean_data_csv, how='outer', indicator=True).query("_merge != 'both'").drop(
        '_merge', axis=1).reset_index(drop=True)

    # Return clean data and dropped data as separated dataframes
    return clean_data_csv, data_csv_dropped


def preprocess_gt_data(gt_data, remove_still_mode):
    """ Preprocess the ground Truth xlsx data file as follows:
        - Remove unnamed columns if exist
        - Change the GT_TimeOrig column to datetime.time
        - Drop rows with NaN on GT_Date or GT_TimeOrig  after data type conversion
        - Create GT_DateTimeCombined column joining GT_Date and GT_TimeOrig columns
        - Assign timezone to GT_DateTimeCombined
        - Add column required to be used as key while merging with ground truth data
        - Generates a log with the dropped rows during the preprocess
    Returns:
        Preprocessed gt dataframe
        :param gt_data: A data frame loaded from a xls file generated manually from GT data collection process
        :param remove_still_mode: boolean value to indicate if records with STILL mode must be removed
    """
    # Drop unnamed columns
    unnamed_cols = [col for col in gt_data.columns if 'Unnamed' in col]
    gt_data = gt_data.drop(unnamed_cols, axis=1)

    # # Remove records with STILL mode if required
    # if remove_still_mode:
    #     gt_data = gt_data[gt_data.GT_Mode != 'STILL']

    # Change data type of TimeOrig to string to make possible date-time casting
    gt_data.GT_TimeOrig = gt_data.GT_TimeOrig.astype(str)
    # Change the GT_TimeOrig column to datetime.time, coerce will produce NaT if the change is not possible
    gt_data['GT_TimeOrig'] = pd.to_datetime(gt_data['GT_TimeOrig'], errors='coerce').dt.time

    # Change data type of GT_TimeDest to string to make possible date-time casting
    gt_data.GT_TimeDest = gt_data.GT_TimeDest.astype(str)
    # Change the GT_TimeDest column to datetime.time, coerce will produce NaT if the change is not possible
    gt_data['GT_TimeDest'] = pd.to_datetime(gt_data['GT_TimeDest'], errors='coerce').dt.time

    # Remove records with STILL mode if required
    if remove_still_mode:
        clean_gt_data = gt_data[gt_data.GT_Mode != 'STILL']

    # Drop rows with NaT on GT_Date or GT_TimeOrig
    clean_gt_data = clean_gt_data.dropna(subset=constants.GT_RELEVANT_COLS_LIST)

    # Add the data to be dropped to a data frame
    data_gt_dropped = pd.merge(gt_data, clean_gt_data, how='outer', indicator=True).query("_merge != 'both'").drop(
        '_merge', axis=1).reset_index(drop=True)

    # Create GT_DateTimeCombined column
    clean_gt_data.loc[:, 'GT_DateTimeCombined'] = clean_gt_data.apply(
        lambda x: datetime.datetime.combine(x.GT_Date, x.GT_TimeOrig), 1)
    # Assign timezone to GT_DateTimeCombined
    clean_gt_data.loc[:, 'GT_DateTimeCombined'] = clean_gt_data.apply(
        lambda x: pytz.timezone(x.GT_TimeZone).localize(x.GT_DateTimeCombined), 1)
    # Add column to be used in function "merge_asoft"
    clean_gt_data.loc[:, 'GT_DateTimeOrigUTC'] = clean_gt_data['GT_DateTimeCombined'].dt.tz_convert('UTC')

    # Create GT_DateTimeDestCombined column
    clean_gt_data.loc[:, 'GT_DateTimeDestCombined'] = clean_gt_data.apply(
        lambda x: datetime.datetime.combine(x.GT_Date, x.GT_TimeDest), 1)
    # Assign timezone to GT_DateTimeDestCombined
    clean_gt_data.loc[:, 'GT_DateTimeDestCombined'] = clean_gt_data.apply(
        lambda x: pytz.timezone(x.GT_TimeZone).localize(x.GT_DateTimeDestCombined), 1)
    # Add column to be used in "merge" functions
    clean_gt_data.loc[:, 'GT_DateTimeDestUTC'] = clean_gt_data['GT_DateTimeDestCombined'].dt.tz_convert('UTC')

    return clean_gt_data, data_gt_dropped
