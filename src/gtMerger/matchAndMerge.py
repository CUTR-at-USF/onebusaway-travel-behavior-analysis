"""
/*
 * Copyright (C) 2019-2021 University of South Florida
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
 """

import datetime
import os
from pathlib import Path

import pandas as pd
import pytz

# Import dependencies
from args import get_parser


# -------------------------------------------

def main():
    # Verify if the OBA input file exists
    if not os.path.isfile(command_line_args.obaFile):
        print("OBA data file not found:", command_line_args.obaFile)
        exit()

    # Verify if GT input file exists
    if not os.path.isfile(command_line_args.gtFile):
        print("Ground truth data file not found:", command_line_args.gtFile)
        exit()

    # Verify if the data folder exists
    if not os.path.isdir(command_line_args.dataDir):
        print("Data folder not found, trying to create it in the current working directory:", command_line_args.dataDir)
        try:
            os.makedirs(command_line_args.dataDir, exist_ok=True)
        except OSError:
            print("There was an error while creating the data folder:", command_line_args.dataDir)
            exit()

    # Create sub-folders for output an logs
    path_logs = os.path.join(command_line_args.dataDir, "logs")
    if not os.path.isdir(path_logs):
        try:
            os.mkdir(path_logs)
        except OSError:
            print("There was an error while creating the sub folder for logs:", path_logs)
            exit()

    path_output = os.path.join(command_line_args.dataDir, "output")
    if not os.path.isdir(path_output):
        try:
            os.mkdir(path_output)
        except OSError:
            print("There was an error while creating the sub-folder for output files:", path_logs)
            exit()

    # Create path OS independent for excel file
    excel_path = Path(command_line_args.gtFile)
    # Load ground truth data to a dataframe
    gt_data = pd.read_excel(excel_path)

    # Preprocess ground truth data
    gt_data = preprocess_gt_data(gt_data)
    print("Ground truth data preprocessed.")

    # Create path OS independent for csv file
    csv_path = Path(command_line_args.obaFile)
    # Load OBA data
    oba_data = pd.read_csv(csv_path)
    # Preprocess OBA data
    oba_data, data_csv_dropped = preprocess_oba_data(oba_data)
    print("OBA data preprocessed.")

    # Data preprocessing is over
    # Save oba dropped data to a csv file
    dropped_file_path = os.path.join(command_line_args.dataDir, "logs", "droppedObaData.csv")
    data_csv_dropped.to_csv(path_or_buf=dropped_file_path, index=False)

    # merge dataframes
    merged_data_frame = merge(gt_data, oba_data)

    # Save merged data to csv
    merged_file_path = os.path.join(command_line_args.dataDir, "output", "mergedData.csv")
    merged_data_frame.to_csv(path_or_buf=merged_file_path, index=False)


def preprocess_oba_data(data_csv) -> object:
    """ Preprocess the csv data file from oba-firebase-export as follows:
        - Change activity start date datatype from str to datetime
        - Drop observations whose activity start date are NaN after data type conversion
        - Drop observations that does not match the time duration and distance requirements from command_line_args
          minActivityDuration and minTripLength
        - Add column required to be used as key while merging with ground truth data
    
    Args:
        data_csv: A data frame loaded from a csv file generated by oba-firebase-export

    Returns:
        Preprocessed dataframe
    """
    # Change Activity Start Date and Time* (UTC) to datetime
    data_csv['Activity Start Date and Time* (UTC)'] = pd.to_datetime(data_csv['Activity Start Date and Time* (UTC)'],
                                                                     errors='coerce', utc=True)

    # Drop NaN rows for relevant columns
    clean_data_csv = data_csv.dropna(
        subset=['Activity Start Date and Time* (UTC)', 'Origin location Date and Time (*best) (UTC)',
                'Duration* (minutes)', 'Origin-Destination Bird-Eye Distance* (meters)'])

    # Remove trips with Duration less than command_line_args.minActivityDuration minutes and distance less than
    # command_line_args.minTripLength
    clean_data_csv = clean_data_csv[(data_csv['Duration* (minutes)'] >= command_line_args.minActivityDuration) & (
            data_csv['Origin-Destination Bird-Eye Distance* (meters)'] >= command_line_args.minTripLength)]

    # Add the data to be dropped to a data frame
    data_csv_dropped = pd.merge(data_csv, clean_data_csv, how='outer', indicator=True).query("_merge != 'both'").drop(
        '_merge', axis=1).reset_index(drop=True)

    # Return clean data and dropped data as separated dataframes
    return clean_data_csv, data_csv_dropped


def preprocess_gt_data(gt_data):
    """ Preprocess the ground Truth xlsx data file as follows:
        - Remove unnamed columns if exist
        - Change the column to datetime.time
        - Drop rows with NaN on GT_Date or GT_TimeOrig  after data type conversion
        - Create GT_DateTimeCombined column joining GT_Date and GT_TimeOrig columns
        - Assign timezone to GT_DateTimeCombined
        - Add column required to be used as key while merging with ground truth data
        - Generates a log with the dropped rows during the preprocess
    
    Args:
        gt_data: A data frame loaded from a xls file generated manually from GT data collection process

    Returns:
        Preprocessed gt dataframe
    """
    # Drop unnamed columns
    unnamed_cols = [col for col in gt_data.columns if 'Unnamed' in col]
    gt_data = gt_data.drop(unnamed_cols, axis=1)

    # Change data type of TimeOrig to string to simplify date time conversions
    gt_data.GT_TimeOrig = gt_data.GT_TimeOrig.astype(str)

    # Change the column to datetime.time, coerce will produce NaN if the change is not possible
    gt_data['GT_TimeOrig'] = pd.to_datetime(gt_data['GT_TimeOrig'], errors='coerce').dt.time

    # Drop rows with NaN on GT_Date or GT_TimeOrig
    clean_gt_data = gt_data.dropna(subset=['GT_Date', 'GT_TimeOrig'])

    # Add the data to be dropped to a data frame
    data_gt_dropped = pd.merge(gt_data, clean_gt_data, how='outer', indicator=True).query("_merge != 'both'").drop(
        '_merge', axis=1).reset_index(drop=True)
    # Save data to be dropped to a csv file
    dropped_file_path = os.path.join(command_line_args.dataDir, "logs", "droppedGtData.csv")
    data_gt_dropped.to_csv(path_or_buf=dropped_file_path, index=False)

    # Create GT_DateTimeCombined column
    clean_gt_data.loc[:, 'GT_DateTimeCombined'] = clean_gt_data.apply(
        lambda x: datetime.datetime.combine(x.GT_Date, x.GT_TimeOrig), 1)

    # Assign timezone to GT_DateTimeCombined
    clean_gt_data.loc[:, 'GT_DateTimeCombined'] = clean_gt_data.apply(
        lambda x: pytz.timezone(x.GT_TimeZone).localize(x.GT_DateTimeCombined), 1)

    # Add column to be used in function "merge_asoft"
    clean_gt_data['ClosestTime'] = clean_gt_data['GT_DateTimeCombined'].dt.tz_convert('UTC')
    return clean_gt_data


def merge(gt_data, oba_data):
    """
    Merge gt_data dataframe and oba_data dataframe using the nearest value between columns 'gt_data.ClosestTime' and
    'oba_data.Activity Start Date and Time* (UTC)'. Before merging, the data is grouped by 'GT_Collector' on gt_data and
    each row on gt_data will be paired with one or none of the rows on oba_data grouped by userId.
    :param gt_data: dataframe with preprocessed data from ground truth XLSX data file
    :param oba_data: dataframe with preprocessed data from OBA firebase export CSV data file
    :return: dataframe with the merged data.
    """
    list_collectors = gt_data['GT_Collector'].unique()
    list_oba_users = oba_data['User ID'].unique()

    merged_df = pd.DataFrame()
    for collector in list_collectors:
        print("Merging data for collector ", collector)
        # Create dataframe for a collector on list_collectors
        gt_data_collector = gt_data[gt_data["GT_Collector"] == collector]
        # Make sure dataframe is sorted by 'ClosesTime'
        gt_data_collector.sort_values('ClosestTime', inplace=True)
        for oba_user in list_oba_users:
            print("\t Oba user", oba_user[-4:])
            # Create a dataframe with the oba_user activities only
            oba_data_user = oba_data[oba_data["User ID"] == oba_user]
            # Make sure dataframes is sorted by 'Activity Start Date and Time* (UTC)'
            oba_data_user.sort_values('Activity Start Date and Time* (UTC)', inplace=True)

            temp_merge = pd.merge_asof(gt_data_collector, oba_data_user, left_on="ClosestTime",
                                       right_on="Activity Start Date and Time* (UTC)",
                                       direction="nearest",
                                       tolerance=pd.Timedelta(str(command_line_args.tolerance) + "ms"))
            merged_df = pd.concat([merged_df, temp_merge], ignore_index=True)
    return merged_df


if __name__ == '__main__':
    command_line_args = get_parser()
    main()
